{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Exercise 11"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"bi3FA4xqKwGjDGNdwxnA7O",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Exercise 11 (no programming)\n",
    "\n",
    "Make the frequency analysis of the text by normalizing all the words in the text.\n",
    "\n",
    "[How to make the task.](https:\/\/courses.openedu.ru\/assets\/courseware\/v1\/84bcb7c4563a6772addcb93e341e0f05\/asset-v1:ITMOUniversity+INTROMLAAISBENG+spring_2023_ITMO_mag+type@asset+block\/Task_1_not_programmers__EN_instruction_.pdf)\n",
    "\n",
    "Click [**the link**](https:\/\/courses.openedu.ru\/assets\/courseware\/v1\/b4c7823727d4aeb75d1e0b5c5355ae85\/asset-v1:ITMOUniversity+INTROMLAAISBENG+spring_2023_ITMO_mag+type@asset+block\/Task_1_EN_not_progr_leskov.txt) to see a piece of the Russian classical literature. Your task is to normalize words using [**the tool**](http:\/\/textanalysisonline.com\/spacy-word-lemmatize) and find the frequencies of the specified words using [**the service**](https:\/\/planetcalc.com\/3205\/). Enter only the frequency of the word itself (neglect the words with attached symbols).\n",
    "\n",
    "**Enter the frequency of the word \"molva\"**\n",
    "- ✅ 6\n",
    "\n",
    " \n",
    "**Enter the frequency of the word \"russia\"**\n",
    "- ✅ 7\n",
    "\n",
    "**Enter the frequency of the word \"carriage\"**\n",
    "- ✅ 6\n",
    " \n",
    "**Enter the frequency of the word \"flea\"**\n",
    "- ✅ 33 \n",
    " "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"KFaf8BipZrIRZXBj9hQ1bV",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Exercise 11 (programming)\n",
    "\n",
    "Click the link to see the task on adding the code parts:\n",
    "\n",
    "Notebook in *.pynb format with some examples of commands to make the exercise.\n",
    "We recommend to use Google colab  to work with Notebook. \n",
    "\n",
    "Your task is to add some parts of code to the example considered. The right answers are in the Notebook. Please proceed to the individual task only when you obtain a correctly functioning code.\n",
    "\n",
    "## Demo Part\n",
    "\n",
    "> For Individual task scroll below"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"tUbMm94rFGxikgR3TgLO8Q",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"4CSx2xYWLqavLvHc3rG6Ve"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "**Data URL**"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"91Xg7IIjj5eosJSd5XSGb3",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"5pqVaUIB66rUzISY2xv2di"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "DATA_URL = \"http:\/\/www.gutenberg.org\/files\/913\/913-0.txt\""
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Ai75VKUaQRhdIuEcmIfW4X",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"1y0k3H2eYXFIpCMdPokw55"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "!pip install -q nltk # ==3.2.5"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Xnjmscz16QZBK8dKXAHtCo",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"xIXG7x6t5kJ1Sq13WQVf0D"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# !cd ..\/\n",
    "# !unzip \/usr\/local\/lib\/python3.10\/dist-packages\/nltk_data\/corpora\/wordnet.zip -d ..\/usr\/local\/lib\/python3.10\/dist-packages\/nltk_data\/corpora\/nltk_data\/corpora\/"
   ],
   "execution_count":null,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "unzip:  cannot find or open ..\/usr\/local\/lib\/python3.10\/dist-packages\/nltk_data\/corpora\/wordnet.zip, ..\/usr\/local\/lib\/python3.10\/dist-packages\/nltk_data\/corpora\/wordnet.zip.zip or ..\/usr\/local\/lib\/python3.10\/dist-packages\/nltk_data\/corpora\/wordnet.zip.ZIP.\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ZWXFyyhujfYYszA2DWMmtW",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"fgTtD2S9RtQwcoQk0gQBrU"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Loading *NLTK*'s 'wordnet'"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"0TvihVcqdSVRxdb75u9Nfd",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"TgsM6oLtHZuu3M1nyi7RzV"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "lemmatizer"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package wordnet to \/home\/datalore\/nltk_data...\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "<WordNetLemmatizer>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"JFELwM3oF41h4jqDJnkJLP",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"uqfpap1hbImWj0j8YpJhNX"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Downloading the text for the task via `urllib`"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"2xH6kjqeg7CRJLOiFaXcGa",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"qAgzDpCXb2DEcVhPyb97sH"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# import urllib.request\n",
    "from urllib.request import build_opener, HTTPCookieProcessor, Request\n",
    "\n",
    "\n",
    "# opener = urllib.request.URLopener({})\n",
    "\n",
    "opener = build_opener(HTTPCookieProcessor())\n",
    "request = Request(DATA_URL)\n",
    "resource = opener.open(request, timeout=30)\n",
    "# content = response.read()\n",
    "\n",
    "\n",
    "# resource = opener.open(DATA_URL)\n",
    "charset = resource.headers.get_content_charset()\n",
    "print(\"Charset\", charset)\n",
    "raw_text = resource.read()\n",
    "\n",
    "# raw_text = urllib.urlopen(DATA_URL).read()\n",
    "\n",
    "if charset:\n",
    "  raw_text = raw_text.decode(resource.headers.get_content_charset())\n",
    "else:\n",
    "  raw_text = raw_text.decode(\"utf-8\")\n",
    "\n",
    "raw_text[:100]"
   ],
   "execution_count":4,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Charset None\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "'\\ufeffThe Project Gutenberg EBook of A Hero of Our Time, by M. Y. Lermontov\\r\\n\\r\\nThis eBook is for the use '"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"7i2laaULiOb2gYYvao4opG",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"QShvTymvc9DHb0isbeNd1j"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import requests\n",
    "\n",
    "response = requests.get(DATA_URL)\n",
    "raw_text = response.text\n",
    "raw_text[:100]"
   ],
   "execution_count":15,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'ï»¿The Project Gutenberg EBook of A Hero of Our Time, by M. Y. Lermontov\\r\\n\\r\\nThis eBook is for the us'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"sbTy5xKdn25MVW5fR0hde2",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"PpEQc1PahFaU0i8iJjxBXw"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Removing the book ending (Gutenberg legal information)"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"B2QFVxRcgadtUOg4GTtI06",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"8A0ewp8fCiRmN5J0Jof0Rj"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import re\n",
    "\n",
    "clean_pattern = re.compile(\"End of the Project Gutenberg EBook.*\")\n",
    "cleaner_text =  re.sub(clean_pattern, \"\", raw_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \"))\n",
    "cleaner_text[-100:]"
   ],
   "execution_count":5,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'ases, equivalent to: “Men are fools, fortune is  blind, and life is not worth a straw.”]            '"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Dr3DoPytUxuxXZqzZdt7dg",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"Fes7bnNeBukjCCOSmXEMN5"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Splitting the text into tokens with a little help from [NLTK](https:\/\/nltk.org\/)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"2z14pl3RPpbY65O2ihHk5T",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"TjMbMLejcUWq5ZqoBcM77N"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens =  word_tokenize(cleaner_text)\n",
    "\"A total of %d 'tokens'\" % len(tokens)"
   ],
   "execution_count":6,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\/punkt.zip.\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "\"A total of 72566 'tokens'\""
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"aT4nc6R3Mpv1seNKYtBYsS",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"LeDehROYFDEBNWHtLkwRiZ"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Now we are about to **lemmatize the tokens**. Please note that for better results we should have first PoS-tagged the text (e.g. with NLTK as well, [please refer to the book and the docs](https:\/\/www.nltk.org\/book\/ch05.html)). `WordNetLemmatizer` would work best with PoS tags provided. However, to make things short and simple, we won't do it as of now."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"RuuVusK2Wnf88VmAuCiTpd",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"aPOYS7LbsWh7lsJrShZqKS"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Task \\#1\n",
    "using Python's standard library's `str.isalpha` modify the code below to remove all non-letter tokens from sentences.\n",
    "\n",
    "---\n"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"rBdXK5AQRdweKKfXiBj4pq",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"zNep4q2hT7eRkDFTGL3ZzM"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\"a1aa\".isalpha()"
   ],
   "execution_count":35,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "False"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"eh4IuVemVngcysLAHPCYfk",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(lemma) for lemma in tqdm(tokens) if lemma.isalpha()]\n",
    "lemmas[-10:]"
   ],
   "execution_count":7,
   "outputs":[
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"b714b12ca206480a8ab5048a80af52aa"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"PkfxUeBqu3wuySfHLvEN2M"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/plain":[
       "['fortune', 'is', 'blind', 'and', 'life', 'is', 'not', 'worth', 'a', 'straw']"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"hYNRa80NmNHeoxfIZjpBr4",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"pb1k5loljGF9pNjYe6oZd7"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Let us check if everything is correct. Total corpus size should be equal to 59087"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"ofu0JzqT9VGuXCxpkPNcD0",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"W5vhxu7eQRHv9YhgHY77BS"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "len(lemmas)"
   ],
   "execution_count":8,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "59087"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"AKN7RKmaOBpdqjNE4Ss7Wd",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"n0bF9IBLiWoBmQh6zK4JuR"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "To continue working on this assignment make sure those numbers match!"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"WinkPoq887zhQKxV76NFI8",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"oA8OdARrkQngQSJCxfyLkf"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "---\n",
    "\n",
    "## Task #2\n",
    "\n",
    "Using `lemmas`, english NLTK stopwords (`nltk.corpus.stopwords`) and `nltk.FreqDist`, compute **the fraction of the stopwords** in the top-100 most frequent words in the text. \n",
    "\n",
    "E.g. if there were just **3 stopwords in 100** most frequent words in the text, the answer would be **0.03**\n",
    "\n",
    "**Googling how to work with NLTK is encouraged.**\n",
    "\n",
    "---\n"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"DI0Fvqcl5UJ0OfVB70N4fT",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"bLU44Mx9AIZoloqINPOnbT"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "stopwords.words(\"english\")[:5]"
   ],
   "execution_count":15,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\/stopwords.zip.\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"4lBbvZ5lP63gMUr3h14GSp",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"yANXSGifOIXm2C2NZdl3C4"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Check yourself: stopwords rate in 100 most frequent words in the text should be 0.66"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"csJxMpcPvxwcV1lVOOylZ5",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"BUtzZnKtvVozYAszUEVWD4"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(lemma for lemma in lemmas)"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"9lgnkrTM0fa2FjnPRkXkSr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "len(fdist)"
   ],
   "execution_count":12,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "5971"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"9Nd5bcq04uHSqkS3h9flmK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cnt = 0\n",
    "for i, val in enumerate(fdist):\n",
    "    if i + 1 == 100:\n",
    "        break\n",
    "    if val in STOPWORDS:\n",
    "        cnt+=1\n",
    "\n",
    "print(f\"{cnt\/100}\")"
   ],
   "execution_count":17,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "0.66\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"z8STZ4EkQa9kqYrmVM7Lxn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Task #3\n",
    "Compute how many tokens occur in the text **strictly greater than** 50 times.\n",
    "\n",
    "---\n"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"j26uZ35a8t8LWaqb1eHS6J",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"kKQ4KKA5tFpB4CRXAIyvcr"
     }
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "mapka = dict()\n",
    "\n",
    "for token in lemmas:\n",
    "    if token not in mapka:\n",
    "        mapka[token] = 0\n",
    "    mapka[token] += 1"
   ],
   "execution_count":26,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"q0f68WcS4ORuTtmoNVC7i3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cnt_gr_50 = 0\n",
    "\n",
    "for val in mapka.values():\n",
    "    if val > 50:\n",
    "        cnt_gr_50 += 1\n",
    "\n",
    "print(f\"{cnt_gr_50}\")"
   ],
   "execution_count":27,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "149\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"EZGHUI0YlFK0mLicWwqrO1",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "Chek yourself: 149"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"OMbokMy04T30ci5ExdPPv8",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "report_properties":{
      "rowId":"HIgPmxnHH4e16UftTtmA99"
     }
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Individual task\n",
    "\n",
    "Click the link to see an example of the Russian classical literature. Your task is to analyze the text.\n",
    "\n",
    "### Helpers"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"AiGJkqwEtW32INiaMkvaIX",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "EX_DATA_URL = \"http:\/\/www.gutenberg.org\/files\/1081\/1081-h\/1081-h.htm\"\n",
    "\n",
    "from urllib.request import build_opener, HTTPCookieProcessor, Request\n",
    "\n",
    "opener = build_opener(HTTPCookieProcessor())\n",
    "request = Request(EX_DATA_URL)\n",
    "resource = opener.open(request, timeout=30)\n",
    "\n",
    "charset = resource.headers.get_content_charset()\n",
    "print(\"Charset\", charset)\n",
    "raw_ex_text = resource.read()\n",
    "\n",
    "if charset:\n",
    "  raw_ex_text = raw_ex_text.decode(resource.headers.get_content_charset())\n",
    "else:\n",
    "  raw_ex_text = raw_ex_text.decode(\"utf-8\")\n",
    "\n",
    "raw_ex_text[:100]"
   ],
   "execution_count":28,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Charset None\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\r\\n\\r\\n<!DOCTYPE html\\r\\n   PUBLIC \"-\/\/W3C\/\/DTD XHTML 1.0 Strict\/\/E'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"lkIouTx2iirzi8wMcOOg3b",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import re\n",
    "\n",
    "clean_pattern = re.compile(\"End of the Project Gutenberg EBook.*\")\n",
    "cleaner_ex_text =  re.sub(clean_pattern, \"\", raw_ex_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \"))\n",
    "cleaner_ex_text[-100:]"
   ],
   "execution_count":29,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'districts.]      <\/p>      <p>        <br \/><br \/>      <\/p>  <pre xml:space=\"preserve\">            '"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"NjC2eNTLGkaQRLtniaNhdf",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "ex_tokens = word_tokenize(cleaner_ex_text)\n",
    "\"A total of %d 'tokens'\" % len(ex_tokens)"
   ],
   "execution_count":31,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "\"A total of 202226 'tokens'\""
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"lO0RIICdP5g1RSdJtOjrXE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Task 1 - Enter the number of letter tokens"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"iHJZ5nWvwmUwue73KEjcav",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "ex_lemmas = [lemmatizer.lemmatize(lemma) for lemma in tqdm(ex_tokens) if lemma.isalpha()]\n",
    "len(ex_lemmas)"
   ],
   "execution_count":32,
   "outputs":[
    {
     "data":{
      "application\/vnd.jupyter.widget-view+json":{
       "version_major":2,
       "version_minor":0,
       "model_id":"3c0f0fa26b7a41f0ac578fe4664f76df"
      }
     },
     "metadata":{
      "application\/vnd.jupyter.widget-view+json":{
       "datalore":{
        "widget_id":"xMatIZWrlqrWDVnXvEIEkL"
       }
      }
     },
     "output_type":"display_data"
    },
    {
     "data":{
      "text\/plain":[
       "151419"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"zzVmymd9U1ic509MLkuTFz",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Task 2 - What is the share of stop-words of 200 most frequent tokens in the text?"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"D7Tzpy0nQQvnSzBmkv6tDn",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "ex_fdist = FreqDist(lemma for lemma in ex_lemmas)"
   ],
   "execution_count":33,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"D98KP2yxmJEmI2CfzWLTs9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "ex_cnt = 0\n",
    "for i, val in enumerate(ex_fdist):\n",
    "    if i + 1 == 200:\n",
    "        break\n",
    "    if val in STOPWORDS:\n",
    "        ex_cnt += 1\n",
    "\n",
    "print(f\"{ex_cnt\/200}\")"
   ],
   "execution_count":34,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "0.45\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"oyXWD3jug86TI6czyi6tVa",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Task 3 - What is the amount of tokens encountered in the text strictly greater than 20 times?"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"9yc2qLQXKQyJq2nrxFT9xK",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "ex_mapka = dict()\n",
    "\n",
    "for token in ex_lemmas:\n",
    "    if token not in ex_mapka:\n",
    "        ex_mapka[token] = 0\n",
    "    ex_mapka[token] += 1"
   ],
   "execution_count":36,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"Hxd93OTorI1wAcMnH6rQv4",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "cnt_gr_20 = 0\n",
    "\n",
    "for val in ex_mapka.values():\n",
    "    if val > 20:\n",
    "        cnt_gr_20 += 1\n",
    "\n",
    "print(f\"{cnt_gr_20}\")"
   ],
   "execution_count":38,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "791\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"LdgT6SJuiy0YThTNq6ueXE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ],
   "report_row_ids":[
    "4CSx2xYWLqavLvHc3rG6Ve",
    "5pqVaUIB66rUzISY2xv2di",
    "1y0k3H2eYXFIpCMdPokw55",
    "xIXG7x6t5kJ1Sq13WQVf0D",
    "fgTtD2S9RtQwcoQk0gQBrU",
    "EnL7ENmBwlqECTseqdSbqX",
    "TgsM6oLtHZuu3M1nyi7RzV",
    "uqfpap1hbImWj0j8YpJhNX",
    "qAgzDpCXb2DEcVhPyb97sH",
    "QShvTymvc9DHb0isbeNd1j",
    "PpEQc1PahFaU0i8iJjxBXw",
    "8A0ewp8fCiRmN5J0Jof0Rj",
    "Fes7bnNeBukjCCOSmXEMN5",
    "TjMbMLejcUWq5ZqoBcM77N",
    "LeDehROYFDEBNWHtLkwRiZ",
    "aPOYS7LbsWh7lsJrShZqKS",
    "GDDDImJNj4YAnaurV1q7kf",
    "rg6JdDoqwPfUTTib7441Pv",
    "zNep4q2hT7eRkDFTGL3ZzM",
    "pb1k5loljGF9pNjYe6oZd7",
    "W5vhxu7eQRHv9YhgHY77BS",
    "n0bF9IBLiWoBmQh6zK4JuR",
    "oA8OdARrkQngQSJCxfyLkf",
    "bLU44Mx9AIZoloqINPOnbT",
    "yANXSGifOIXm2C2NZdl3C4",
    "BUtzZnKtvVozYAszUEVWD4",
    "kKQ4KKA5tFpB4CRXAIyvcr",
    "HIgPmxnHH4e16UftTtmA99"
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}